{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036069af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textblob import TextBlob\n",
    "from nltk import download\n",
    "# If necessary, download the required NLTK packages --> \n",
    "nltk.download('punkt')\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data\n",
    "df = pd.read_csv('classified_comments.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca40f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate n-grams\n",
    "# Build your stopword set (English, Spanish, Russian, + custom)\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "stop_words_es = set(stopwords.words('spanish'))\n",
    "stop_words_ru = set([\n",
    "    'и','в','во','не','что','он','на','я','с','со','как','а','то',\n",
    "    'все','она','так','его','но','да','ты','к','у','же','вы','за',\n",
    "    'бы','по','ее','мне','было','вот','от','меня','еще','нет','о',\n",
    "    'из','ему','теперь','когда','даже','ну','вдруг','ли','если',\n",
    "    'уже','или','ни','быть','был','него','до','вас','нибудь',\n",
    "    'опять','уж','вам','ведь','там','потом','себя','ничего','ей',\n",
    "    'может','они','тут','где','есть','надо','ней','для','мы',\n",
    "    'тебя','их','чем','была','сам','чтоб','без','будто','чего',\n",
    "    'раз','тоже','себе','под','будет','ж','тогда','кто','этот',\n",
    "    'того','потому','этого','какой','совсем','ним','здесь',\n",
    "    'этом','один','почти','мой','тем','чтобы','нее','сейчас',\n",
    "    'были','куда','зачем','всех','никогда','можно','при','наконец',\n",
    "    'два','об','другой','хоть','после','над','больше','тот',\n",
    "    'через','эти','нас','про','всего','них','какая','много',\n",
    "    'разве','три','эту','моя','впрочем','хорошо','свою','этой',\n",
    "    'перед','иногда','лучше','чуть','том','нельзя','такой',\n",
    "    'им','более','всегда','конечно','всю','между'\n",
    "])\n",
    "custom_stopwords = set([\n",
    "        'game', 'video', 'youtube', 'comment', 'player', \"yeah\", \"im\", \"na\", \"yet\", \"a\", \"one\", \"oh\", \"isnt\", \"didnt\",\n",
    "        'games', 'play', 'like', \"u\", \"c\", \"jim\", \"stopvivekbindra\"\n",
    "    ])  \n",
    "\n",
    "stop_words = stop_words_en | stop_words_es | stop_words_ru | custom_stopwords\n",
    "\n",
    "# Updated n-gram function\n",
    "def generate_ngrams(text, n):\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return []  # Handle empty comments\n",
    "    \n",
    "    # Tokenize and lowercase\n",
    "    words = word_tokenize(text.lower())\n",
    "    \n",
    "    # Filter stopwords and non-alphabetic tokens\n",
    "    words = [w for w in words if w.isalpha() and w not in stop_words]\n",
    "    \n",
    "    # Generate n-grams\n",
    "    return list(ngrams(words, n))\n",
    "\n",
    "# This will generate lists of n-grams for each comment\n",
    "df['2-grams'] = df['comment'].apply(lambda x: generate_ngrams(x, 2))\n",
    "df['3-grams'] = df['comment'].apply(lambda x: generate_ngrams(x, 3))\n",
    "df['4-grams'] = df['comment'].apply(lambda x: generate_ngrams(x, 4))\n",
    "\n",
    "# Explode the lists of n-grams into individual rows for counting\n",
    "all_2grams = df['2-grams'].explode().dropna()\n",
    "all_3grams = df['3-grams'].explode().dropna()\n",
    "all_4grams = df['4-grams'].explode().dropna()\n",
    "# Convert n-grams from tuple to string format (so they can be counted)\n",
    "all_2grams = all_2grams.apply(lambda x: ' '.join(x))\n",
    "all_3grams = all_3grams.apply(lambda x: ' '.join(x))\n",
    "all_4grams = all_4grams.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30e340ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the most common n-grams\n",
    "counter_2grams = Counter(all_2grams)\n",
    "counter_3grams = Counter(all_3grams)\n",
    "counter_4grams = Counter(all_4grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0c708e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the counter objects\n",
    "df_2grams = pd.DataFrame(counter_2grams.most_common(100), columns=['2-gram', 'Frequency'])\n",
    "df_3grams = pd.DataFrame(counter_3grams.most_common(100), columns=['3-gram', 'Frequency'])\n",
    "df_4grams = pd.DataFrame(counter_4grams.most_common(100), columns=['4-gram', 'Frequency'])\n",
    "\n",
    "# Optionally, save the DataFrames as CSVs\n",
    "df_2grams.to_csv('top_2grams.csv', index=False)\n",
    "df_3grams.to_csv('top_3grams.csv', index=False)\n",
    "df_4grams.to_csv('top_4grams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c1221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_table_image(df, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(6, len(df)*0.5))  # Auto-scale height\n",
    "    ax.axis('off')\n",
    "    table = ax.table(\n",
    "        cellText=df.values,\n",
    "        colLabels=df.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center'\n",
    "    )\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 1.5)\n",
    "    plt.title(title, fontsize=14, pad=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Example for top 10 n-grams:\n",
    "make_table_image(df_2grams.head(10), \"Top 2-grams\", \"top_2grams.png\")\n",
    "make_table_image(df_3grams.head(10), \"Top 3-grams\", \"top_3grams.png\")\n",
    "make_table_image(df_4grams.head(10), \"Top 4-grams\", \"top_4grams.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b229219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a new CSV\n",
    "df.to_csv('n-gram_youtube_comments.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad19da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words across all comments: 1249752\n"
     ]
    }
   ],
   "source": [
    "# Function to count words in a comment\n",
    "def count_words(text):\n",
    "    if pd.isna(text) or len(text.strip()) == 0:\n",
    "        return 0  # Return 0 if comment is empty or NaN\n",
    "    words = word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "df['word_count'] = df['comment'].apply(count_words)\n",
    "\n",
    "# Get the total word count across all comments\n",
    "total_word_count = df['word_count'].sum()\n",
    "print(f'Total number of words across all comments: {total_word_count}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
