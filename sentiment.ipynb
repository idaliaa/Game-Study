{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdaa7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# ---------- Setup ----------\n",
    "# Make sure VADER lexicon is available\n",
    "try:\n",
    "    SentimentIntensityAnalyzer()\n",
    "except LookupError:\n",
    "    nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edf9375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load CSV data\n",
    "df = pd.read_csv(\"n-gram_youtube_comments.csv\", engine=\"python\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23ca9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Sentiment analysis\n",
    "def vader_scores(text: str):\n",
    "    return sia.polarity_scores(text)\n",
    "\n",
    "scores = df['comment'].apply(vader_scores).apply(pd.Series)\n",
    "df = pd.concat([df, scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd4579b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map compound score â†’ sentiment label\n",
    "def label_from_compound(c):\n",
    "    if c >= 0.05:\n",
    "        return \"positive\"\n",
    "    elif c <= -0.05:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "df['sentiment_label'] = df['compound'].apply(label_from_compound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac2213",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Aggregate insights\n",
    "label_share = (df[\"sentiment_label\"].value_counts(normalize=True) * 100).round(2)\n",
    "avg_compound = df[\"compound\"].mean()\n",
    "\n",
    "print(\"=== Aggregate ===\")\n",
    "print(\"Distribution (%):\")\n",
    "print(label_share)\n",
    "print(\"Average compound:\", round(avg_compound, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d24078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top most positive/negative examples\n",
    "top_pos = df.nlargest(5, \"compound\")[[\"comment\", \"compound\"]]\n",
    "top_neg = df.nsmallest(5, \"compound\")[[\"comment\", \"compound\"]]\n",
    "\n",
    "print(\"\\nTop positive examples:\")\n",
    "print(top_pos.to_string(index=False))\n",
    "print(\"\\nTop negative examples:\")\n",
    "print(top_neg.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b894eccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level analysis (Top tokens & bigrams per sentiment)\n",
    "def top_ngrams(texts, ngram_range=(1,1), top_k=20):\n",
    "    vec = CountVectorizer(stop_words=\"english\", lowercase=True,\n",
    "                          ngram_range=ngram_range, min_df=2)\n",
    "    X = vec.fit_transform(texts)\n",
    "    freqs = np.asarray(X.sum(axis=0)).ravel()\n",
    "    vocab = np.array(vec.get_feature_names_out())\n",
    "    order = freqs.argsort()[::-1][:top_k]\n",
    "    return pd.DataFrame({\"ngram\": vocab[order], \"count\": freqs[order]})\n",
    "\n",
    "pos_texts = df.loc[df[\"sentiment_label\"]==\"positive\", \"comment\"]\n",
    "neg_texts = df.loc[df[\"sentiment_label\"]==\"negative\", \"comment\"]\n",
    "\n",
    "top_pos_unigrams = top_ngrams(pos_texts, (1,1), top_k=20)\n",
    "top_neg_unigrams = top_ngrams(neg_texts, (1,1), top_k=20)\n",
    "\n",
    "top_pos_bigrams  = top_ngrams(pos_texts, (2,2), top_k=20)\n",
    "top_neg_bigrams  = top_ngrams(neg_texts, (2,2), top_k=20)\n",
    "\n",
    "top_pos_threegrams  = top_ngrams(pos_texts, (3,3), top_k=20)\n",
    "top_neg_threegrams  = top_ngrams(neg_texts, (3,3), top_k=20)\n",
    "\n",
    "top_pos_fourgrams  = top_ngrams(pos_texts, (4,4), top_k=20)\n",
    "top_neg_fourgrams  = top_ngrams(neg_texts, (4,4), top_k=20)\n",
    "\n",
    "print(\"\\nTop positive unigrams:\\n\", top_pos_unigrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0733508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop negative unigrams:\\n\", top_neg_unigrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7286dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop positive bigrams:\\n\", top_pos_bigrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d965129",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop negative bigrams:\\n\", top_neg_bigrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b614528",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop positive threegrams:\\n\", top_pos_threegrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72914ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop negative threerams:\\n\", top_neg_threegrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop positive fourrams:\\n\", top_pos_fourgrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b514e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTop negative fourrams:\\n\", top_neg_fourgrams.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e982da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence & intensity\n",
    "# (Bucket by compound magnitude)\n",
    "def intensity_bucket(c):\n",
    "    m = abs(c)\n",
    "    if m >= 0.7:  return \"very strong\"\n",
    "    if m >= 0.4:  return \"strong\"\n",
    "    if m >= 0.2:  return \"moderate\"\n",
    "    if m >  0.0:  return \"weak\"\n",
    "    return \"neutral/zero\"\n",
    "\n",
    "df[\"intensity\"] = df[\"compound\"].apply(intensity_bucket)\n",
    "\n",
    "print(\"\\nIntensity distribution (%):\")\n",
    "print((df[\"intensity\"].value_counts(normalize=True)*100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c4335f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced modeling (Topics)\n",
    "# LDA topics -> then average sentiment per topic\n",
    "# Build a sparse term matrix for topics\n",
    "topic_vec = CountVectorizer(stop_words=\"english\", lowercase=True, min_df=5, max_df=0.5)\n",
    "X = topic_vec.fit_transform(df[\"comment\"])\n",
    "\n",
    "n_topics = 8  # adjust as you like\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42, learning_method=\"batch\")\n",
    "topic_dist = lda.fit_transform(X)            # shape: (n_docs, n_topics)\n",
    "df[\"topic\"] = topic_dist.argmax(axis=1)      # dominant topic per comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a06aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top words per topic\n",
    "terms = np.array(topic_vec.get_feature_names_out())\n",
    "def top_words_for_topic(topic_idx, top_k=12):\n",
    "    comp = lda.components_[topic_idx]\n",
    "    return terms[comp.argsort()[::-1][:top_k]]\n",
    "\n",
    "print(\"\\n=== Topics & average sentiment ===\")\n",
    "topic_summary = []\n",
    "for k in range(n_topics):\n",
    "    words = \", \".join(top_words_for_topic(k, 10))\n",
    "    avg_c = df.loc[df[\"topic\"]==k, \"compound\"].mean()\n",
    "    cnt   = (df[\"topic\"]==k).sum()\n",
    "    topic_summary.append((k, cnt, round(avg_c, 4), words))\n",
    "topic_df = pd.DataFrame(topic_summary, columns=[\"topic\",\"n\",\"avg_compound\",\"top_words\"])\n",
    "print(topic_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization (Matplotlib)\n",
    "# 7a) Bar: sentiment label distribution\n",
    "label_order = [\"negative\",\"neutral\",\"positive\"]\n",
    "label_counts = df[\"sentiment_label\"].value_counts().reindex(label_order, fill_value=0)\n",
    "\n",
    "plt.figure()\n",
    "label_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Sentiment distribution\")\n",
    "plt.xlabel(\"Sentiment\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44d70dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7b) Histogram: compound score\n",
    "plt.figure()\n",
    "df[\"compound\"].plot(kind=\"hist\", bins=50)\n",
    "plt.title(\"Compound score distribution\")\n",
    "plt.xlabel(\"Compound\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfcc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7c) Bar: intensity buckets\n",
    "plt.figure()\n",
    "df[\"intensity\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Sentiment intensity buckets\")\n",
    "plt.xlabel(\"Intensity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f0b8f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved: sentiment_youtube_comments.csv\n",
      "sentiment_label\n",
      "neutral     39138\n",
      "positive    36764\n",
      "negative    14183\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "df.to_csv(\"sentiment_youtube_comments.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved: sentiment_youtube_comments.csv\")\n",
    "print(df['sentiment_label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
