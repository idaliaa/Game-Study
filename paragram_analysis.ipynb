{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cde591",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAGRAM ANALYSIS #####\n",
    "# Step-by-Step Guide for Paragram Analysis\n",
    "# We will break this into manageable steps:\n",
    "# Preprocessing the Comments\n",
    "# Training or Using Pretrained Word2Vec / GloVe Embeddings\n",
    "# Semantic Similarity Calculation (Comparing Sentences)\n",
    "# Clustering or Grouping Similar Comments\n",
    "# Visualizing or Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eac3a539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/psylviana/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# Load your cleaned dataset (assuming the file is named 'cleaned_youtube_comments.csv')\n",
    "df = pd.read_csv('cleaned_youtube_comments.csv')\n",
    "# Tokenize the comments in the 'comment' column (assuming they are already cleaned)\n",
    "df['tokenized_comments'] = df['comment'].apply(lambda x: nltk.word_tokenize(x))  # Tokenizing without converting to lowercase\n",
    "\n",
    "# Train the Word2Vec model using the tokenized comments\n",
    "model = Word2Vec(sentences=df['tokenized_comments'], vector_size=200, window=10, min_count=5, workers=4)\n",
    "\n",
    "# Save the trained model for later use\n",
    "model.save(\"word2vec_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cfe18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas display options to avoid truncating text in columns\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Avoid truncating text\n",
    "pd.set_option('display.width', 1000)        # Increase the total display width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3875c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load the previously trained Word2Vec model\n",
    "model = Word2Vec.load(\"word2vec_model\")\n",
    "\n",
    "# Function to get the vector representation of a comment\n",
    "def get_comment_vector(comment, model):\n",
    "    # Get the word vectors for each word in the comment\n",
    "    vectors = [model.wv[word] for word in comment if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)  # Average the word vectors\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)  # Return a zero vector if no word is in the model\n",
    "\n",
    "# Convert each tokenized comment into its vector representation\n",
    "df['comment_vector'] = df['tokenized_comments'].apply(lambda x: get_comment_vector(x, model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94910c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample a smaller subset (e.g., 100 comments) to avoid memory overload\n",
    "sample_df = df.sample(n=100, random_state=42)  # Adjust n as needed\n",
    "\n",
    "# Compute pairwise cosine similarity for the smaller subset\n",
    "similarity_matrix = cosine_similarity(list(sample_df['comment_vector']))\n",
    "\n",
    "# Example: Get similarity between the first comment and all other comments in the sample\n",
    "print(similarity_matrix[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aefbef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                comment  cluster\n",
      "32066  i waited for the day this video would pop up in my sub feed and yet it took me by surprise im so happy right now        3\n",
      "10203                                                                                                               036        1\n",
      "6395                                                                after 10 years this will become old so enjoy it now        3\n",
      "5665                                                                                                 137 hits hard geez        1\n",
      "27419                                                                                                    jumpscare 2822        1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "\n",
    "# Sample a subset (e.g., 100 comments) for faster clustering\n",
    "sample_df = df.sample(n=100, random_state=42)  # Adjust n as needed\n",
    "\n",
    "# Choose the number of clusters (adjust as needed based on dataset)\n",
    "num_clusters = 5  # For example, adjust based on your data\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Apply clustering to the comment vectors (from the sampled data)\n",
    "sample_df['cluster'] = kmeans.fit_predict(list(sample_df['comment_vector']))\n",
    "\n",
    "# Show the cluster assignments for the first few comments\n",
    "print(sample_df[['comment', 'cluster']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9faa00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Apply KMeans clustering to your comment vectors (make sure it's run)\n",
    "# First, ensure you have 'comment_vector' column created (e.g., from Word2Vec embeddings)\n",
    "\n",
    "# Choose the number of clusters (adjust based on your dataset)\n",
    "num_clusters = 8  # Adjust this based on your data\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Apply KMeans clustering to the comment vectors\n",
    "df['cluster'] = kmeans.fit_predict(list(df['comment_vector']))\n",
    "\n",
    "# Step 2: Reduce dimensionality using PCA for visualization\n",
    "# Optionally, sample a smaller subset of data (to avoid memory issues)\n",
    "sample_df = df.sample(n=1000, random_state=42)  # Adjust n as needed\n",
    "\n",
    "# PCA for dimensionality reduction (reduce to 2D for visualization)\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(list(sample_df['comment_vector']))\n",
    "\n",
    "# Step 3: Create a scatter plot of the PCA results, colored by cluster\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(pca_results[:, 0], pca_results[:, 1], c=sample_df['cluster'], cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.title(\"PCA Visualization of Comment Clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "514d5f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                             comment  cluster\n",
      "0                                 if you enjoy gamespub videos a comment like or sub would be highly appreciated it means a lot for us you can also follow us on twitter for updates        5\n",
      "1                                                                                                                                                                obrigado pela ajuda        2\n",
      "2   i am german and i hated franz kafkas books at school in my childhood when you are 12 years old you really dont get whats going on but seems german education system wants you to        5\n",
      "3                                                                                           thank you for your help on the ammo puzzle honestly didnt know where to go with that one        5\n",
      "4                                                                                                         whoa whoa whoawhos still listening in 2023 we have a sick spotify playlist        3\n",
      "5                                                                                                                                          answer this comment when it says 1 yr ago        5\n",
      "6                                                                                                                                                                      1likes1pushup        2\n",
      "7                                            i want it there on youtube forever at lease t allow us to download it we will sync it to pen drives with old songs that never come back        5\n",
      "8                                                                                                                                                                               2025        2\n",
      "9                                                                                                                                                                 this song the best        4\n",
      "10                                                                                                                                                                     011 beat drop        3\n",
      "11                                                                                                                                                                             crazy        3\n",
      "12                                                                                           this is one of those phonk songs where someone doesnt scream fart or burp into the mike        7\n",
      "13                                                                                                                                                                           massive        2\n",
      "14                                                                                                                                                                        miscall 99        2\n",
      "15                                                                                                                                                         what a melody anyone 2025        3\n",
      "16                                                                                                                                                      this song reminds me of 2023        4\n",
      "17                                                                                                                                                                              nice        3\n",
      "18                                                                                                                                                                  кто из 2025 года        2\n",
      "19                                                                                                                                 still sounds nostalgic here from some sigma edits        3\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the comment_vector column is created before clustering\n",
    "# Assuming 'comment_vector' has already been created using Word2Vec or other method\n",
    "\n",
    "# Choose the number of clusters (adjust as needed based on your data)\n",
    "num_clusters = 8  # You can adjust this value\n",
    "\n",
    "# Apply KMeans clustering to the comment vectors\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df['cluster'] = kmeans.fit_predict(list(df['comment_vector']))\n",
    "\n",
    "# Create a new DataFrame with comments and their corresponding cluster labels\n",
    "cluster_table = df[['comment', 'cluster']]\n",
    "\n",
    "# Display the cluster table (first 10 rows for preview)\n",
    "print(cluster_table.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kafka-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
